<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Times New Roman:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"./public/search.xml"};
  </script>

  <meta name="description" content="Shi-Xue Zhang &amp;diams; 张世学 Google Scholar &amp;starf; GitHub &amp;starf; Semantic Scholar  Studying for Ph.D on Computer Vision and Pattern Recognition  PRIR Lab, University of Science and Technology Beiji">
<meta property="og:type" content="website">
<meta property="og:title" content="Academic Resume">
<meta property="og:url" content="http://example.com/scholar/index.html">
<meta property="og:site_name" content="S.X.Zhang">
<meta property="og:description" content="Shi-Xue Zhang &amp;diams; 张世学 Google Scholar &amp;starf; GitHub &amp;starf; Semantic Scholar  Studying for Ph.D on Computer Vision and Pattern Recognition  PRIR Lab, University of Science and Technology Beiji">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/scholar/src/my_photo.jpg">
<meta property="og:image" content="http://example.com/scholar/src/USTB_logo.png">
<meta property="og:image" content="http://example.com/scholar/src/cvpr2020/img2.png">
<meta property="og:image" content="http://example.com/scholar/src/cvpr2020/img1.png">
<meta property="og:image" content="http://example.com/scholar/src/iccv2021/framework.png">
<meta property="og:image" content="http://example.com/scholar/src/iccv2021/intro.png">
<meta property="og:image" content="http://example.com/scholar/src/TNNLS2022/framework.png">
<meta property="og:image" content="http://example.com/scholar/src/TNNLS2022/intro.png">
<meta property="og:image" content="http://example.com/scholar/src/APIN2022/framework.png">
<meta property="og:image" content="http://example.com/scholar/src/APIN2022/intro.png">
<meta property="og:image" content="http://example.com/scholar/src/TPAMI2022/framework.png">
<meta property="og:image" content="http://example.com/scholar/src/TPAMI2022/intro.png">
<meta property="og:image" content="http://example.com/scholar/src/TMM2023/framework.png">
<meta property="og:image" content="http://example.com/scholar/src/TMM2023/intro.png">
<meta property="og:image" content="http://example.com/scholar/src/CICAI2022/framework.png">
<meta property="og:image" content="http://example.com/scholar/src/CICAI2022/intro.png">
<meta property="article:published_time" content="2021-04-20T13:43:05.000Z">
<meta property="article:modified_time" content="2023-06-15T16:33:50.181Z">
<meta property="article:author" content="Shi-Xue Zhang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/scholar/src/my_photo.jpg">

<link rel="canonical" href="http://example.com/scholar/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Academic Resume | S.X.Zhang
</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?9fa61c0bedabef52b57c662e236ef33b";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="S.X.Zhang" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">S.X.Zhang</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-scholar">

    <a href="/scholar/" rel="section"><i class="fa fa-trophy fa-fw"></i>学术</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
  
  

          <div class="content page posts-expand">
            

    
    
    
    <div class="post-block" lang="zh-CN">
      <header class="post-header">

<h1 class="post-title" itemprop="name headline">Academic Resume
</h1>

<div class="post-meta">
  

</div>

</header>

      
      
      
      <div class="post-body">
          <meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=1.0,minimum-scale=1.0,user-scalable=no">

 <p><img src="./src/my_photo.jpg" width=180 height=240 alt="My Photo"></p>
<center><b>Shi-Xue Zhang &diams; 张世学</b></center>
<center><a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=N8jMnXEAAAAJ&hl=en"><b>Google Scholar</b></a> &starf; <a target="_blank" rel="noopener" href="https://github.com/GXYM"><b>GitHub</b></a> &starf; <a target="_blank" rel="noopener" href="https://www.semanticscholar.org/author/Shi-Xue-Zhang/3235252"><b>Semantic Scholar</b></a> </center>
<p><center>Studying for Ph.D on Computer Vision and Pattern Recognition <br/>
PRIR Lab, University of Science and Technology Beijing</center></p>


<h2 id="News"><a href="#News" class="headerlink" title="News"></a>News</h2><style>
   .timeline-small {
            max-width: 350px;
            max-height: 630px;
            overflow: hidden;
            margin: 30px auto 0;
            box-shadow: 0 0 40px #a0a0a0;
            font-family: 'Open Sans', sans-serif;
        }
 .timeline-small-body ul {
            padding: 1rem 0rem 0rem 1rem;
            margin: 0rem;
            list-style: none;
            position: relative;
        }
 .timeline-small-body ul::before {
            content: '';
            height: 95%;
            width: 0.2rem;
            background-color: #d9d9d9;
            position: absolute;
            top: 0rem;
            left: 1.275rem;
            z-index: -1;
        }
.timeline-small-body li div {
            display: inline-block;
            margin: 0rem 0rem;
            vertical-align: top;
        }
.timeline-small-body .bullet {
            width: 0.75rem;
            height: 0.75rem;
            box-sizing: border-box;
            border-radius: 50%;
            background: #fff;
            z-index: 1;
            margin-top: 0.75rem;
                   margin-right: 0.2rem;
                   float:left;
        }
.timeline-small-body .bullet.pink {
            background-color: hotpink;
            border: 0.2rem solid #F93B69;
        }
 .timeline-small-body .bullet.gray {
            background-color: lightGrey ;
            border: 0.2rem solid lightGrey ;
        }
.timeline-small-body .bullet.green {
            background-color: lightseagreen;
            border: 0.2rem solid #B0E8E2;
        }
 .timeline-small-body .bullet.blue {
            background-color: aquamarine;
            border: 0.2rem solid cadetblue;
        }
 .timeline-small-body .bullet.orange {
            background-color: salmon;
            border: 0.2rem solid #EB8B6E;
        }
 .timeline-small-body .date {
            width: 20%;
            font-size: 0.85rem;
            padding-top: 0.4rem;
            padding-right: 0.5rem;
        }
  .timeline-small-body .desc {
            width: 70%;
        }
  .timeline-small-body h3 {
            font-size: 1rem;
            font-weight: 0.9rem;
            margin: 0rem;
        }
 .timeline-small-body h4 {
            margin: 0rem;
            font-size: 0.8rem;
            font-weight: 0.9rem;
            color: #808080;
        }
</style>
 <!-- 主要内容部分 -->
<div class="timeline-small-body">
  <ul>
    <!--
    <li>
      <div class="bullet pink"></div>
      <div class="date">2020/04/23</div>
      <div class="desc">
        <h3>更新！</h3>
        <h4>网站待完善中。。。。</h4>
      </div>
     </li>
     -->
    <li>
      <div class="bullet blue"></div>
      <div class="date">2023/06/12</div>
      <div class="desc">
        <h3>TMM 2023</h3>
        <h4>My paper "<a href="#tmm23">Arbitrary Shape Text Detection via Boundary Transformer</a>" has been accepted by IEEE Transactions on Multimedia (TMM) 2023!</h4>
      </div>
     </li>
    <li>
      <div class="bullet pink"></div>
      <div class="date">2023/04/18</div>
      <div class="desc">
        <h3>2023年腾讯犀牛鸟精英人才计划</h3>
        <h4>入选2023年腾讯犀牛鸟精英人才计划, 2023年全球共有55名同学入选该计划。</h4>
      </div>
     </li>
    <li>
      <div class="bullet pink"></div>
      <div class="date">2022/12/23</div>
      <div class="desc">
        <h3>PRCV 2022</h3>
        <h4>Awarded CCF-CV Fellowship (CCF-CV 学术新锐奖), Only 3 students in China were awarded for their research in computer vision every year.</h4>
      </div>
     </li>
    <li>
      <div class="bullet gray"></div>
      <div class="date">2022/09/05</div>
      <div class="desc">
        <h3>CICAI 2022</h3>
        <h4>Our paper "<a href="#cicai22">Scene Text Recognition with Single-Point
Decoding Network</a>" has been accepted by CAAI International Conference on Artificial Intelligence 2022!</h4>
      </div>
     </li>
    <li>
      <div class="bullet blue"></div>
      <div class="date">2022/05/16</div>
      <div class="desc">
        <h3>TPAMI 2022</h3>
        <h4>My paper "<a href="#pami22">Arbitrary Shape Text Detection via Segmentation with Probability Maps</a>" has been accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2022!</h4>
      </div>
     </li>
    <li>
      <div class="bullet blue"></div>
      <div class="date">2022/02/16</div>
      <div class="desc">
        <h3>APIN 2022</h3>
        <h4>My paper "<a href="#apin22">Graph Fusion Network for Multi-Oriented Object Detection</a>" has been accepted by Applied Intelligence (APIN) 2022!</h4>
      </div>
     </li>
    <li>
      <div class="bullet blue"></div>
      <div class="date">2022/02/15</div>
      <div class="desc">
        <h3>TNNLS 2022</h3>
        <h4>My paper "<a href="#tnnls22">Kernel Proposal Network for Arbitrary Shape Text Detection</a>" has been accepted by TNNLS 2022!</h4>
      </div>
     </li>
    <li>
      <div class="bullet blue"></div>
      <div class="date">2021/07/23</div>
      <div class="desc">
        <h3>ICCV 2021</h3>
        <h4>My paper "<a href="#iccv21">Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection</a>" has been accepted by ICCV 2021 (Poster)!</h4>
      </div>
     </li>
     <li>
       <div class="bullet blue"></div>
       <div class="date">2020/02/27</div>
       <div class="desc">
         <h3>CVPR 2020</h3>
         <h4>My paper "<a href="#cvpr20">Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection</a>" has been accepted by CVPR 2020 (Oral)!</h4>
       </div>
     </li>
  </ul>
</div>

<h2 id="Education"><a href="#Education" class="headerlink" title="Education"></a>Education</h2><style>
ul,p{padding:0;}
li{list-style:none;}
.box{width:95%;margin:0rem auto;}
.box .img{float:left;}
.box .img img{width:8rem;height:8rem;margin-right: 5rem;}
.box .list{display:inline;white-space:normal;}
.box .list li{line-height:2.0rem;font-size:1.0rem;list-style-type:circle}
</style>

<div class="box">
   <div class="img">
        <img src="./src/USTB_logo.png" />
   </div>
   <ul class="list">
     <li>2021/09~Now: PhD on Computer Science and Technology. PRIR Lab, University of Science and Technology Beijing.</li>
     <li>2018/09~2021/01: Master Degree on Computer Science and Technology. PRIR Lab, University of Science and Technology Beijing.</li>
     <li >2014/09~2018/06: Bachelor Degree on IoT Engineering. School of Computer and Communication Engineering, University of Science and Technology Beijing.</li>
</ul> 
</div>


<h2 id="Work-Experiences"><a href="#Work-Experiences" class="headerlink" title="Work Experiences"></a>Work Experiences</h2><style>
ul,p{padding:0;}
.list_s{display:inline;white-space:normal;}
.list_s li{line-height:1.5rem;font-size:0.95rem;list-style-type:circle;margin-left: 1.2rem;}
</style>
<div class="list_s">
<li>2023/05~Now: Internship on Data Algorithm Research at TEG (Advertising Multimedia AI Center), Tencent Technology (Shenzhen) Co. Ltd.</li>
</div>



<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p> 1.<b>Shi-Xue Zhang</b>, Xiaobin Zhu, Jie-Bo Hou, Chang Liu, Chun Yang, Hongfa Wang, Xu-Cheng Yin,”Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection”,CVPR 2020 Oral,CCF A.<br> 2.<b>Shi-Xue Zhang</b>, Xiaobin Zhu, Chun Yang, Hongfa Wang, Xu-Cheng Yin, “Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection”, ICCV 2021 Poster, CCF A.<br> 3.<b>Shi-Xue Zhang</b>, Xiaobin Zhu, Lei Chen, Jie-Bo Hou, Xu-Cheng Yin,”Arbitrary Shape Text Detection via Segmentation with Probability Maps”, T-PAMI 2022, JCR一区, CCF A.<br> 4.<b>Shi-Xue Zhang</b>, Xiaobin Zhu, Jie-Bo Hou, Chun Yang, Xu-Cheng Yin,”Kernel Proposal Network for Arbitrary Shape Text Detection”,TNNLS 2022, JCR一区, CCF B.<br> 5.<b>Shi-Xue Zhang</b>, Chun Yang, Xiaobin Zhu, Xu-Cheng Yin,”Arbitrary Shape Text Detection via Boundary Transformer”，TMM 2023, JCR一区, CCF B.<br> 6.<b>Shi-Xue Zhang</b>, Xiaobin Zhu, Jie-Bo Hou, Xu-Cheng Yin,”Graph Fusion Network for Multi-Oriented Object Detection”，Applied Intelligence (APIN), JCR二区, CCF C.<br> 7.Lei Chen, Haibo Qin, <b>Shi-Xue Zhang</b>, Chun Yang, Xu-Cheng Yin ,”Scene Text Recognition with Single-Point Decoding Network”. CICAI 2022. (EI).</p>
<h2 id="学业及学术奖励（Academic-Award）"><a href="#学业及学术奖励（Academic-Award）" class="headerlink" title="学业及学术奖励（Academic Award）"></a>学业及学术奖励（Academic Award）</h2><h3 id="博士（Ph-D）"><a href="#博士（Ph-D）" class="headerlink" title="博士（Ph.D）"></a>博士（Ph.D）</h3><style>
ul,p{padding:0;}
.list_s{display:inline;white-space:normal;}
.list_s li{line-height:1.5rem;font-size:0.95rem;list-style-type:circle;margin-left: 1.2rem;}
</style>
<div class="list_s">
<li>2022年12月; 2022年CCF-CV学术新锐奖（全国每年不超过三人）.</li>
<li>2022年12月; 博士研究生国家奖学金.</li>
<li>2022年12月; 北京科技大学优秀三好研究生.</li>
</div>

<h3 id="硕士-M-D"><a href="#硕士-M-D" class="headerlink" title="硕士(M.D)"></a>硕士(M.D)</h3><style>
ul,p{padding:0;}
.list_s{display:inline;white-space:normal;}
.list_s li{line-height:1.5rem;font-size:0.95rem;list-style-type:circle;margin-left: 1.2rem;}
</style>
<div class="list_s">
<li>2021年01月; 北京市优秀硕士毕业生.</li>
<li>2020年12月; 硕士研究生国家奖学金.</li>
<li>2020年12月; 北京科技大学优秀三好研究生.</li>
</div>

<h3 id="本科-B-E"><a href="#本科-B-E" class="headerlink" title="本科((B.E.)"></a>本科((B.E.)</h3><style>
ul,p{padding:0;}
.list_b{display:inline;white-space:normal;}
.list_b li{line-height:1.5rem;font-size:0.95rem;list-style-type:circle;margin-left: 1.2rem;}
</style>
<div class="list_b">
<li>2018年06月; 北京科技大学三好毕业生.</li>
<li>2017年11月; 北京科技大学人民奖学金.</li>
<li>2016年11月; 北京科技大学优秀三好学生.</li>
<li>2016年11月; 国家励志奖学金.</li>    
<li>2015年11月; 国家励志奖学金.</li>    
<li>2015年05月; 北京科技大学新生人民奖学金.</li>  
</div>


<h2 id="竞赛奖励（Competition-Award）"><a href="#竞赛奖励（Competition-Award）" class="headerlink" title="竞赛奖励（Competition Award）"></a>竞赛奖励（Competition Award）</h2><style>
ul,p{padding:0;}
.list_c{display:inline;white-space:normal;}
.list_c li{line-height:1.5rem;font-size:0.95rem;list-style-type:circle;margin-left: 1.2rem;}
</style>
<div class="list_c">
  <li>2020/11;“华为杯”第十七届中国研究生数学建模竞赛；国家级二等奖.</li>
  <li>2017/08; 第十二届全国大学生“恩智浦”杯智能汽车竞赛四旋翼导航组; 国家级二等奖.</li>
  <li>2017/08; 第十二届全国大学生“恩智浦”杯智能汽车竞赛摄像头四轮组; 华北赛亚军-省部级一等奖.</li>
  <li>2017/09; 博尔杯.全国大学生创新大赛; 优胜奖.</li>
  <li>2016/12;“共享杯”大学生科技资源共享服务创新大赛; 国家级三等奖.</li>
  <li>2016/10; 第十届全国大学生iCAN创新创业大赛; 北京市二等奖.</li>
  <li>2016/05; 首都高校第八届机械创新设计大赛; 北京市二等奖.</li>
  <li>2016/12; 全国大学生数学建模竞赛北京赛区甲组;北京市二等奖.</li>
  <li>2016/08; 北京市电子设计大赛; 北京市三等奖.</li>
  <li>2015/12; 全国部分地区大学生物理竞赛; 北京市三等奖.</li>
  <li>2015/08; 第九届全国大学生iCAN创新创业校内赛; 校级三等奖.</li>  
</div>

<hr>
<hr>
<hr>
<p id="cvpr20"></p>

<h2 id="Main-Publications"><a href="#Main-Publications" class="headerlink" title="Main Publications"></a>Main Publications</h2><h3 id="1-Deep-Relational-Reasoning-Graph-Network-for-Arbitrary-Shape-Text-Detection-CVPR2020-Oral"><a href="#1-Deep-Relational-Reasoning-Graph-Network-for-Arbitrary-Shape-Text-Detection-CVPR2020-Oral" class="headerlink" title="1. Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection (CVPR2020 Oral)"></a>1. Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection (CVPR2020 Oral)</h3><p>Shi-Xue Zhang, Xiaobin Zhu, Jie-Bo Hou, Chang Liu, Chun Yang, Hongfa Wang, Xu-Cheng Yin, “Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection ”，CVPR 2020: 9696-9705.</p>
 <p><img src="./src/cvpr2020/img2.png" alt="framework"></p>
 
<style>
.image{float:right;width:30%;}
.image img{width:95%;min-width:5rem;max-width:25rem;margin-left: 1rem; margin-top: 2.5rem;}
</style>
<div>
<div class="image">
    <img src="./src/cvpr2020/img1.png"/>
</div>
  <p style="line-height: 2.0"><br><strong>Abstract</strong> <i>Arbitrary shape text detection is a challenging task due to the high variety and complexity of scenes texts. In this paper, we propose a novel unified relational reasoning graph network for arbitrary shape text detection. In our method, an innovative local graph bridges a text proposal model via Convolutional Neural Network (CNN) and a deep relational reasoning network via Graph Convolutional Network (GCN), making our network end-to-end trainable. To be concrete, every text instance will be divided into a series of small rectangular components, and the geometry attributes (\eg, height, width, and orientation) of the small components will be estimated by our text proposal model. Given the geometry attributes, the local graph construction model can roughly establish linkages between different text components. For further reasoning and deducing the likelihood of linkages between the component and its neighbors, we adopt a graph-based network to perform deep relational reasoning on local graphs. Experiments on public available datasets demonstrate the state-of-the-art performance of our method.</i><br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2003.07493">Paper:arXiv</a>], [<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9157700/">Paper:IEEE</a>],[<a href="https://github.com/GXYM/DRRG" target="_blank" rel="noopener">Code:gitHub</a>]</p>
</div>
<p id="iccv21"></p>

<h3 id="2-Adaptive-Boundary-Proposal-Network-for-Arbitrary-Shape-Text-Detection-ICCV2021-Poster"><a href="#2-Adaptive-Boundary-Proposal-Network-for-Arbitrary-Shape-Text-Detection-ICCV2021-Poster" class="headerlink" title="2. Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection (ICCV2021 Poster)"></a>2. Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection (ICCV2021 Poster)</h3><p>Shi-Xue Zhang, Xiaobin Zhu, Chun Yang, Hongfa Wang, Xu-Cheng Yin, “Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection ”，ICCV 2021: 1285-1294.</p>
 <p><img src="./src/iccv2021/framework.png" alt="framework"></p>
 
<style>
.image{float:right;width:30%;}
.image img{width:95%;min-width:5rem;max-width:25rem;margin-left: 1rem; margin-top: 2.5rem;}
</style>
<div>
<div class="image">
    <img src="./src/iccv2021/intro.png"/>
</div>
  <p style="line-height: 2.5"><br><strong>Abstract</strong> <i>Arbitrary shape text detection is a challenging task due to the high complexity and  variety of scene texts. In this work, we propose a novel adaptive boundary proposal network for arbitrary shape text detection,  which can learn to directly produce accurate boundary for arbitrary shape text without any post-processing. Our method mainly consists of a boundary proposal model and an innovative adaptive boundary deformation model. The boundary proposal model constructed by multi-layer dilated convolutions is adopted to produce prior information (including classification map, distance field, and direction field) and coarse boundary proposals. The adaptive boundary deformation model is an encoder-decoder network, in which the encoder mainly consists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network (RNN). It aims to perform boundary deformation in an  iterative way for obtaining  text instance shape guided by prior information from the boundary proposal model. In this way, our method can directly and efficiently generate accurate text boundaries without complex post-processing. Extensive experiments on publicly available datasets demonstrate the state-of-the-art performance of our method.</i> <br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2107.12664">Paper:arXiv</a>],[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9711486">Paper:IEEE</a>], [<a href="https://github.com/GXYM/TextBPN" target="_blank" rel="noopener">Code:gitHub</a>]</p>
</div>
<p id="tnnls22"></p>

<h3 id="3-Kernel-Proposal-Network-for-Arbitrary-Shape-Text-Detection-TNNLS-2022"><a href="#3-Kernel-Proposal-Network-for-Arbitrary-Shape-Text-Detection-TNNLS-2022" class="headerlink" title="3. Kernel Proposal Network for Arbitrary Shape Text Detection (TNNLS 2022)"></a>3. Kernel Proposal Network for Arbitrary Shape Text Detection (TNNLS 2022)</h3><p>Shi-Xue Zhang, Xiaobin Zhu, Jie-Bo Hou, Chun Yang, Xu-Cheng Yin,“Kernel Proposal Network for Arbitrary Shape Text Detection”,TNNLS 2022, JCR一区.</p>
 <p><img src="./src/TNNLS2022/framework.png" alt="framework"></p>
 
<style>
.image{float:right;width:30%;}
.image img{width:95%;min-width:5rem;max-width:25rem;margin-left: 1rem; margin-top: 2.5rem;}
</style>
<div>
<div class="image">
    <img src="./src/TNNLS2022/intro.png"/>
</div>
  <p style="line-height: 2.5"><br><strong>Abstract</strong> <i>Segmentation-based methods have achieved great success for arbitrary shape text detection. However, separating neighboring text instances is still one of the most challenging problems due to the complexity of texts in scene images. In this paper, we propose an innovative Kernel Proposal Network (dubbed KPN) for arbitrary shape text detection. The proposed KPN can separate neighboring text instances by classifying different texts into instance-independent feature maps, meanwhile avoiding the complex aggregation process existing in segmentation-based arbitrary shape text detection methods. To be concrete, our KPN will predict a Gaussian center map for each text image, which will be used to extract a series of candidate kernel proposals (i.e., dynamic convolution kernel) from the embedding feature maps according to their corresponding keypoint positions. To enforce the independence between kernel proposals, we propose a novel orthogonal learning loss (OLL) via orthogonal constraints. Specifically, our kernel proposals contain important self-information learned by network and location information by position embedding. Finally, kernel proposals will individually convolve all embedding feature maps for generating individual embedded maps of text instances. In this way, our KPN can effectively separate neighboring text instances and improve the robustness against unclear boundaries. To our knowledge, our work is the first to introduce the dynamic convolution kernel strategy to efficiently and effectively tackle the adhesion problem of neighboring text instances in text detection. Experimental results on challenging datasets verify the impressive performance and efficiency of our method.</i> <br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.06410">Paper:arXiv</a>],[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9732893">Paper:IEEE</a>], [<a href="https://github.com/GXYM/KPN" target="_blank" rel="noopener">Code:gitHub</a>]</p>
</div>
<p id="apin22"> </p>

<h3 id="4-Graph-Fusion-Network-for-Multi-Oriented-Object-Detection-APIN-2022"><a href="#4-Graph-Fusion-Network-for-Multi-Oriented-Object-Detection-APIN-2022" class="headerlink" title="4.Graph Fusion Network for Multi-Oriented Object Detection (APIN 2022)"></a>4.Graph Fusion Network for Multi-Oriented Object Detection (APIN 2022)</h3><p>Shi-Xue Zhang, Xiaobin Zhu, Jie-Bo Hou, Xu-Cheng Yin, “Graph Fusion Network for Multi-Oriented Object Detection”，Applied Intelligence (APIN)，JCR二区.</p>
 <p><img src="./src/APIN2022/framework.png" alt="framework"></p>
 
<style>
.image{float:right;width:30%;}
.image img{width:95%;min-width:5rem;max-width:25rem;margin-left: 1rem; margin-top: 2.5rem;}
</style>
<div>
<div class="image">
    <img src="./src/APIN2022/intro.png"/>
</div>
  <p style="line-height: 2.5"><br><strong>Abstract</strong> <i>In object detection, non-maximum suppression (NMS) methods are extensively adopted to remove horizontal duplicates of detected dense boxes for generating final object instances. However, due to the degraded quality of dense detection boxes and not explicit exploration of the context information, existing NMS methods via simple intersection-over-union (IoU) metrics tend to underperform on multi-oriented and long-size objects detection. Distinguishing with general NMS methods via duplicate removal, we propose a novel graph fusion network, named GFNet, for multi-oriented object detection. Our GFNet is extensible and adaptively fuse dense detection boxes to detect more accurate and holistic multi-oriented object instances. Specifically, we first adopt a locality-aware clustering algorithm to group dense detection boxes into different clusters. We will construct an instance sub-graph for the detection boxes belonging to one cluster. Then, we propose a  graph-based fusion network via Graph Convolutional Network (GCN) to learn to reason and fuse the detection boxes for generating final instance boxes. Extensive experiments both on public available multi-oriented text datasets (including MSRA-TD500, ICDAR2015, ICDAR2017-MLT) and multi-oriented object datasets (DOTA) verify the effectiveness and robustness of our method against general NMS methods in multi-oriented object detection.</i> <br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.03562.">Paper:arXiv</a>],[<a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s10489-022-03396-5">Paper:APIN</a>],</p>
</div>
<p id="pami22"></p>

<h3 id="5-Arbitrary-Shape-Text-Detection-via-Segmentation-with-Probability-Maps-TPAMI-2022"><a href="#5-Arbitrary-Shape-Text-Detection-via-Segmentation-with-Probability-Maps-TPAMI-2022" class="headerlink" title="5. Arbitrary Shape Text Detection via Segmentation with Probability Maps (TPAMI 2022)"></a>5. Arbitrary Shape Text Detection via Segmentation with Probability Maps (TPAMI 2022)</h3><p>Shi-Xue Zhang, Xiaobin Zhu, Lei Chen, Jie-Bo Hou, Xu-Cheng Yin, “Arbitrary Shape Text Detection via Segmentation with Probability Maps”, T-PAMI 2022，CCF A，JCR一区.</p>
 <p><img src="./src/TPAMI2022/framework.png" alt="framework"></p>
 
<style>
.image{float:right;width:30%;}
.image img{width:95%;min-width:5rem;max-width:25rem;margin-left: 1rem; margin-top: 2.5rem;}
</style>
<div>
<div class="image">
    <img src="./src/TPAMI2022/intro.png"/>
</div>
  <p style="line-height: 2.5"><br><strong>Abstract</strong> <i>Arbitrary shape text detection is a challenging task due to the significantly varied sizes and aspect ratios, arbitrary orientations or shapes, inaccurate annotations, etc. Due to the scalability of pixel-level prediction, segmentation-based methods can adapt to various shape texts and hence attracted considerable attention recently. However, accurate pixel-level annotations of texts are formidable, and the existing datasets for scene text detection only provide coarse-grained boundary annotations. Consequently, numerous misclassified text pixels or background pixels inside annotations always exist, degrading the performance of segmentation-based text detection methods. Generally speaking, whether a pixel belongs to text or not is highly related to the distance with the adjacent annotation boundary. With this observation, in this paper, we propose an innovative and robust segmentation-based detection method via probability maps for accurately detecting text instances. To be concrete, we adopt a Sigmoid Alpha Function (SAF) to transfer the distances between boundaries and their inside pixels to a probability map. However, one probability map can not cover complex probability distributions well because of the uncertainty of coarse-grained text boundary annotations. Therefore, we adopt a group of probability maps computed by a series of Sigmoid Alpha Functions to describe the possible probability distributions. In addition, we propose an iterative model to learn to predict and assimilate probability maps for providing enough information to reconstruct text instances. Finally, simple region growth algorithms are adopted to aggregate probability maps to complete text instances. Experimental results demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy on several benchmarks.
Notably, our method with Watershed Algorithm as post-processing achieves the best F-measure on Total-Text (88.79%), CTW1500 (85.75%), and MSRA-TD500 (88.93%). Besides, our method achieves promising performance on multi-oriented datasets (ICDAR2015) and multilingual datasets (ICDAR2017-MLT).</i> <br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.12419.pdf">Paper:arXiv</a>],[<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/9779460">Paper:IEEE</a>], [<a href="https://github.com/GXYM/TextPMs" target="_blank" rel="noopener">Code:gitHub</a>]</p>
</div>

<p id="tmm23"></p>

<h3 id="6-Arbitrary-Shape-Text-Detection-via-Boundary-Transformer-TMM-2023"><a href="#6-Arbitrary-Shape-Text-Detection-via-Boundary-Transformer-TMM-2023" class="headerlink" title="6. Arbitrary Shape Text Detection via Boundary Transformer (TMM 2023)"></a>6. Arbitrary Shape Text Detection via Boundary Transformer (TMM 2023)</h3><p> <b>Shi-Xue Zhang</b>, Chun Yang, Xiaobin Zhu, Xu-Cheng Yin,”Arbitrary Shape Text Detection via Boundary Transformer”，IEEE Transactions on Multimedia (TMM), JCR一区, CCF B.<br> <p><img src="./src/TMM2023/framework.png" alt="framework"></p></p>
<style>
.image{float:right;width:30%;}
.image img{width:95%;min-width:5rem;max-width:25rem; margin-left: 1rem; margin-top: 2.8rem;}
</style>
<div>
<div class="image">
    <img src="./src/TMM2023/intro.png"/>
</div>
  <p style="line-height: 2.0"><br><strong>Abstract</strong> <i> In arbitrary shape text detection, locating accurate text boundaries is challenging and non-trivial.  
  Existing methods often suffer from indirect text boundary modeling or complex post-processing. 
  In this paper, we systematically present a unified coarse-to-fine framework via boundary learning for arbitrary shape text detection, which can accurately and efficiently locate text boundaries without post-processing.In our method, we explicitly model the text boundary via an innovative iterative boundary transformer in a coarse-to-fine manner. In this way, our method can directly gain accurate text boundaries and abandon complex post-processing to improve efficiency. Specifically, our method mainly consists of a feature extraction backbone, a boundary proposal module, and an iteratively optimized boundary transformer module. The boundary proposal module consisting of multi-layer dilated convolutions will predict important prior information (including classification map, distance field, and direction field) for generating coarse boundary proposals while guiding the boundary transformer's optimization. The boundary transformer module adopts an encoder-decoder structure, in which the encoder is constructed by multi-layer transformer blocks with residual connection while the decoder is a simple multi-layer perceptron network (MLP). Under the guidance of prior information, the boundary transformer module will gradually refine the coarse boundary proposals via iterative boundary deformation. Furthermore, we propose a novel boundary energy loss (BEL) that introduces an energy minimization constraint and an energy monotonically decreasing constraint to further optimize and stabilize the learning of boundary refinement. Extensive experiments on publicly available and challenging datasets demonstrate the state-of-the-art performance and promising efficiency of our method. The code and model are available at: https://github.com/GXYM/TextBPN-Puls-Plus.</i> <br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.05320">Paper:arXiv</a>],[<a href="https://github.com/GXYM/TextBPN-Plus-Plus" target="_blank" rel="noopener">Code:gitHub</a>]</p>
</div>
 
<p id="cicai22"></p>

<h3 id="7-Scene-Text-Recognition-with-Single-Point-Decoding-Network-CICAI-2022"><a href="#7-Scene-Text-Recognition-with-Single-Point-Decoding-Network-CICAI-2022" class="headerlink" title="7. Scene Text Recognition with Single-Point Decoding Network (CICAI 2022)"></a>7. Scene Text Recognition with Single-Point Decoding Network (CICAI 2022)</h3><p>Lei Chen, Haibo Qin, <b>Shi-Xue Zhang</b>, Chun Yang, Xu-Cheng Yin ,”Scene Text Recognition with Single-Point Decoding Network”. CICAI 2022. (EI).</p>
 <p><img src="./src/CICAI2022/framework.png" alt="framework"></p>
 
<style>
.image{float:right;width:40%;}
.image img{width:95%;min-width:5rem;max-width:25rem;margin-left: 1rem; margin-top: 2.5rem;}
</style>
<div>
<div class="image">
    <img src="./src/CICAI2022/intro.png"/>
</div>
  <p style="line-height: 2.5"><br><strong>Abstract</strong> <i> In recent years, attention-based scene text recognition methods have been very popular and attracted the interest of many researchers. Attention-based methods can adaptively focus attention on a small area or even single point during decoding, in which the attention matrix is nearly one-hot distribution. Furthermore, the whole feature maps will be weighted and summed by all attention matrices during inference, causing huge redundant computations. In this paper, we propose an efficient attention-free Single-Point Decoding Network (dubbed SPDN) for scene text recognition, which can replace the traditional attention-based decoding network. Specifically, we propose Single-Point Sampling Module (SPSM) to efficiently sample one key point on the feature map for decoding one character. In this way, our method can not only precisely locate the key point of each character but also remove redundant computations. Based on SPSM, we design an efficient and novel single-point decoding network to replace the attention-based decoding network. Extensive experiments on publicly available benchmarks verify that our SPDN can greatly improve decoding efficiency without sacrificing performance.</i> <br></p>

<p>Resources: [<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.01914.pdf">Paper:arXiv</a>],[<a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-031-20497-5_12">Paper:Springer</a>]</p>
</div>
      </div>
      
      
      
    </div>
    

    
    
    


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#News"><span class="nav-number">1.</span> <span class="nav-text">News</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.1.</span> <span class="nav-text">TMM 2023</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.1.1.</span> <span class="nav-text">My paper &quot;Arbitrary Shape Text Detection via Boundary Transformer&quot; has been accepted by IEEE Transactions on Multimedia (TMM) 2023!</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.2.</span> <span class="nav-text">2023年腾讯犀牛鸟精英人才计划</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.2.1.</span> <span class="nav-text">入选2023年腾讯犀牛鸟精英人才计划, 2023年全球共有55名同学入选该计划。</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.3.</span> <span class="nav-text">PRCV 2022</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.3.1.</span> <span class="nav-text">Awarded CCF-CV Fellowship (CCF-CV 学术新锐奖), Only 3 students in China were awarded for their research in computer vision every year.</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.4.</span> <span class="nav-text">CICAI 2022</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.4.1.</span> <span class="nav-text">Our paper &quot;Scene Text Recognition with Single-Point
Decoding Network&quot; has been accepted by CAAI International Conference on Artificial Intelligence 2022!</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.5.</span> <span class="nav-text">TPAMI 2022</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.5.1.</span> <span class="nav-text">My paper &quot;Arbitrary Shape Text Detection via Segmentation with Probability Maps&quot; has been accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2022!</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.6.</span> <span class="nav-text">APIN 2022</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.6.1.</span> <span class="nav-text">My paper &quot;Graph Fusion Network for Multi-Oriented Object Detection&quot; has been accepted by Applied Intelligence (APIN) 2022!</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.7.</span> <span class="nav-text">TNNLS 2022</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.7.1.</span> <span class="nav-text">My paper &quot;Kernel Proposal Network for Arbitrary Shape Text Detection&quot; has been accepted by TNNLS 2022!</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.8.</span> <span class="nav-text">ICCV 2021</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.8.1.</span> <span class="nav-text">My paper &quot;Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection&quot; has been accepted by ICCV 2021 (Poster)!</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">1.9.</span> <span class="nav-text">CVPR 2020</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">1.9.1.</span> <span class="nav-text">My paper &quot;Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection&quot; has been accepted by CVPR 2020 (Oral)!</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Education"><span class="nav-number">2.</span> <span class="nav-text">Education</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Work-Experiences"><span class="nav-number">3.</span> <span class="nav-text">Work Experiences</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-Work"><span class="nav-number">4.</span> <span class="nav-text">Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B8%9A%E5%8F%8A%E5%AD%A6%E6%9C%AF%E5%A5%96%E5%8A%B1%EF%BC%88Academic-Award%EF%BC%89"><span class="nav-number">5.</span> <span class="nav-text">学业及学术奖励（Academic Award）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%9A%E5%A3%AB%EF%BC%88Ph-D%EF%BC%89"><span class="nav-number">5.1.</span> <span class="nav-text">博士（Ph.D）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%95%E5%A3%AB-M-D"><span class="nav-number">5.2.</span> <span class="nav-text">硕士(M.D)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%AC%E7%A7%91-B-E"><span class="nav-number">5.3.</span> <span class="nav-text">本科((B.E.)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AB%9E%E8%B5%9B%E5%A5%96%E5%8A%B1%EF%BC%88Competition-Award%EF%BC%89"><span class="nav-number">6.</span> <span class="nav-text">竞赛奖励（Competition Award）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Main-Publications"><span class="nav-number">7.</span> <span class="nav-text">Main Publications</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Deep-Relational-Reasoning-Graph-Network-for-Arbitrary-Shape-Text-Detection-CVPR2020-Oral"><span class="nav-number">7.1.</span> <span class="nav-text">1. Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection (CVPR2020 Oral)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Adaptive-Boundary-Proposal-Network-for-Arbitrary-Shape-Text-Detection-ICCV2021-Poster"><span class="nav-number">7.2.</span> <span class="nav-text">2. Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection (ICCV2021 Poster)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Kernel-Proposal-Network-for-Arbitrary-Shape-Text-Detection-TNNLS-2022"><span class="nav-number">7.3.</span> <span class="nav-text">3. Kernel Proposal Network for Arbitrary Shape Text Detection (TNNLS 2022)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Graph-Fusion-Network-for-Multi-Oriented-Object-Detection-APIN-2022"><span class="nav-number">7.4.</span> <span class="nav-text">4.Graph Fusion Network for Multi-Oriented Object Detection (APIN 2022)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Arbitrary-Shape-Text-Detection-via-Segmentation-with-Probability-Maps-TPAMI-2022"><span class="nav-number">7.5.</span> <span class="nav-text">5. Arbitrary Shape Text Detection via Segmentation with Probability Maps (TPAMI 2022)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Arbitrary-Shape-Text-Detection-via-Boundary-Transformer-TMM-2023"><span class="nav-number">7.6.</span> <span class="nav-text">6. Arbitrary Shape Text Detection via Boundary Transformer (TMM 2023)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-Scene-Text-Recognition-with-Single-Point-Decoding-Network-CICAI-2022"><span class="nav-number">7.7.</span> <span class="nav-text">7. Scene Text Recognition with Single-Point Decoding Network (CICAI 2022)</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Shi-Xue Zhang"
      src="/images/img/avatar.jpeg">
  <p class="site-author-name" itemprop="name">Shi-Xue Zhang</p>
  <div class="site-description" itemprop="description">博主</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/GXYM" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;GXYM" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.semanticscholar.org/author/S.-Zhang/3235252" title="S-Scholar → https:&#x2F;&#x2F;www.semanticscholar.org&#x2F;author&#x2F;S.-Zhang&#x2F;3235252" rel="noopener" target="_blank"><i class="fab fa-leanpub fa-fw"></i>S-Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhangshixue111@163.com" title="E-Mail → mailto:zhangshixue111@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fas fa-rss fa-fw"></i>RSS</a>
      </span>
  </div>

<div id="music163player">
<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=100% height=75 src="//music.163.com/outchain/player?type=2&id=38592976&auto=1&height=66"></iframe>
</div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://leetcode-cn.com/problemset/all/" title="https:&#x2F;&#x2F;leetcode-cn.com&#x2F;problemset&#x2F;all&#x2F;" rel="noopener" target="_blank">Leetcode</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">S.X.Zhang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">9k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">8 分钟</span>
</div>

<div class="powered-by">
  <i class="fa fa-user-md"></i>
  <span>
	本站总访客数:<span id="busuanzi_value_site_uv"></span>&nbsp;&nbsp;|
  </span>
  <i class="fa fa-eye"></i>
  <span>
      本站总访问量<span id="busuanzi_value_site_pv"></span>次
  </span>
</div>

<div>
<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("09/09/2020 00:00:00");//在此处修改你的建站时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "已运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
    <span class="post-meta-divider">|</span>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":180,"height":180},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
